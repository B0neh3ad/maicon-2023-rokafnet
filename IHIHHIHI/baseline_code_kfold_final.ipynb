{"cells":[{"cell_type":"code","source":["# 필요한 라이브러리를 설치합니다.\n","! pip install segmentation_models_pytorch\n","! pip install wandb -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWWya25RCpsZ","executionInfo":{"status":"ok","timestamp":1700474395833,"user_tz":-540,"elapsed":19263,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"5116d9f7-b2aa-4dad-ae2b-bd4fe9717d68"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m102.4/106.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu118)\n","Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm==0.9.2 (from segmentation_models_pytorch)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu118)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.19.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=aabdf5b7509f6e5dd0b5117f7376bd68e57aea924565739029d86468b678a8af\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=4103a909dc2c0330fdfc9df0a6f55c38308b06ff730eac921e8031bdec996202\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06dqd_qnCtQw","executionInfo":{"status":"ok","timestamp":1700474354805,"user_tz":-540,"elapsed":37935,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"08557146-a4e0-47c7-b39d-c10c22aec296"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"ILMnPBxTB8fW","executionInfo":{"status":"ok","timestamp":1700474467503,"user_tz":-540,"elapsed":49524,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"ffce0761-2596-4620-aea8-fc336c5b1535"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["# 필요한 라이브러리들을 임포트합니다.\n","from glob import glob\n","import os\n","# Log in to your W&B account\n","import wandb\n","import segmentation_models_pytorch as smp\n","\n","wandb.login()"]},{"cell_type":"code","source":["TRAIN_NAME = \"test\""],"metadata":{"id":"atdwds2puwJd","executionInfo":{"status":"ok","timestamp":1700467494794,"user_tz":-540,"elapsed":6,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j6sq5jfB8fa"},"source":["### 데이터 경로를 설정합니다."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iIkeB47RB8fc","executionInfo":{"status":"ok","timestamp":1700474476152,"user_tz":-540,"elapsed":650,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"outputs":[],"source":["# 데이터 경로를 설정해줍니다.\n","data_directory = '/content/drive/MyDrive/Crack_Segmentation/dataset'\n","\n","# 학습용 데이터 경로와 마스크 경로를 설정해줍니다.\n","train_image_directory = os.path.join(data_directory, 'train', 'images') # 학습용 이미지 경로\n","train_mask_directory = os.path.join(data_directory, 'train', 'masks') # 학습용 마스크 경로\n","\n","# 테스트용 데이터 경로와 마스크 경로를 설정해줍니다.\n","test_image_directory = os.path.join(data_directory, 'test', 'images') # 테스트용 이미지 경로"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvMJoqdWB8fd","executionInfo":{"status":"ok","timestamp":1700474577765,"user_tz":-540,"elapsed":99374,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"d2b586ae-e167-49ca-9528-d0386b5e5edf"},"outputs":[{"output_type":"stream","name":"stdout","text":["학습용 이미지 개수 : 5986, 마스크 개수 : 5986\n","평가용 이미지 개수 : 3992\n"]}],"source":["# 각 경로에 존재하는 파일들의 경로를 리스트로 저장합니다.\n","train_image_paths = sorted(glob(os.path.join(train_image_directory, '*.jpg')))\n","train_mask_paths = sorted(glob(os.path.join(train_mask_directory, '*.png')))\n","\n","test_image_paths = sorted(glob(os.path.join(test_image_directory, '*.jpg')))\n","\n","# 각 이미지와 마스크의 수량을 체크하는 코드입니다.\n","print(f'학습용 이미지 개수 : {len(train_image_paths)}, 마스크 개수 : {len(train_mask_paths)}')\n","print(f'평가용 이미지 개수 : {len(test_image_paths)}')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJFk5eCtB8fd","executionInfo":{"status":"ok","timestamp":1700467567504,"user_tz":-540,"elapsed":15,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"590115a9-ec70-4a0b-de69-63463115bdb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["이미지 경로 예시 : ['/content/drive/MyDrive/Crack_Segmentation/dataset/train/images/train_00000.jpg', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/images/train_00001.jpg', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/images/train_00002.jpg', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/images/train_00003.jpg', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/images/train_00004.jpg']\n","마스크 경로 예시 : ['/content/drive/MyDrive/Crack_Segmentation/dataset/train/masks/train_00000.png', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/masks/train_00001.png', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/masks/train_00002.png', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/masks/train_00003.png', '/content/drive/MyDrive/Crack_Segmentation/dataset/train/masks/train_00004.png']\n"]}],"source":["# 이미지와 마스크의 경로 예시를 보고 혹시 파일 리스트가 순서대로 정렬되어 있지 않는지 확인해봅니다.\n","print(f'이미지 경로 예시 : {train_image_paths[:5]}')\n","print(f'마스크 경로 예시 : {train_mask_paths[:5]}')"]},{"cell_type":"markdown","metadata":{"id":"zJtiLIvpB8fe"},"source":["### Pytorch Dataset 클래스 정의\n","Pytorch Dataset 클래스는 AI 모델에 데이터를 입력하기 전에 데이터를 가공 및 표준화하는 역할을 합니다."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YJlhu0oFB8fe","executionInfo":{"status":"ok","timestamp":1700474900753,"user_tz":-540,"elapsed":2,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"outputs":[],"source":["# 필요한 라이브러리들을 임포트합니다.\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"w6viWROhB8ff","executionInfo":{"status":"ok","timestamp":1700467567505,"user_tz":-540,"elapsed":12,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"outputs":[],"source":["class SegDataset(Dataset):\n","    def __init__(self, image_paths, mask_paths=None, resize=(256, 256), mode='train'):\n","        '''\n","        image_paths: 이미지 경로들의 리스트\n","        mask_paths: 마스크 경로들의 리스트\n","        size: 이미지와 마스크를 리사이즈를 몇으로 할 지 결정하는 변수\n","        mode: train인지 test인지를 결정하는 변수\n","        '''\n","        self.image_paths = image_paths # 이미지 경로들의 리스트를 저장합니다.\n","        self.mask_paths = mask_paths # 마스크 경로들의 리스트를 저장합니다.\n","        self.resize = resize # 리사이즈 변수를 저장합니다.\n","        self.mode = mode # 모드 변수를 저장합니다.\n","        self.transform = transforms.Compose([\n","                transforms.ToTensor(), # 텐서 변환\n","                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화\n","            ]) # 이미지를 텐서로 변환하고 정규화하는 코드입니다.\n","\n","    def __len__(self):\n","        return len(self.image_paths) # 데이터셋의 길이를 반환합니다.\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx] # idx 번째 이미지 경로를 불러옵니다. ex) 전체 이미지 중 첫 번째 이미지\n","        filename = os.path.basename(image_path) # 이미지 경로에서 이미지 파일의 이름을 불러옵니다. ex) train_0001.png\n","\n","        # image를 불러옵니다.\n","        image = Image.open(image_path) # 이미지 경로를 참조해 이미지를 불러옵니다. BGR 형태의 이미지입니다.\n","        image = image.convert('RGB') # RGB 형태로 변환합니다.\n","        image_size = torch.tensor(image.size) # 이미지의 사이즈를 저장합니다.\n","\n","        # image를 resize합니다.\n","        image = image.resize(self.resize)\n","\n","        # image를 torch tensor로 변환합니다.\n","        image = self.transform(image).float() # 딥러닝 모델의 입력은 float형이어야 합니다.\n","\n","        if self.mode == 'train' or self.mode == 'val':\n","            mask_path = self.mask_paths[idx] # idx 번째 마스크 경로를 불러옵니다.\n","\n","            # mask를 로드합니다.\n","            mask = Image.open(mask_path) # 마스크 경로를 참조해 마스크(정답)를 불러옵니다. Gray 이미지이며, 0부터 59 사이의 값을 가집니다.\n","\n","            # mask를 resize합니다.\n","            # 마스크 이미지는 resize 할때 iterpolation 방식에 각별히 주의해야 합니다.\n","            # linear interpolation 등을 사용하면 유효하지 않은 mask 값이 될 수 있습니다.\n","            # 따라서 주변에 있는 픽셀 값으로 interpolation 해야합니다.\n","            mask = mask.resize(self.resize, resample=Image.NEAREST) # 마스크를 리사이징합니다.\n","\n","            # mask를 torch tensor로 변환합니다.\n","            mask = torch.from_numpy(np.array(mask)).long() # 정답은 정수형이어야 하므로 long형으로 변환합니다.\n","\n","        else:\n","            mask = np.zeros(self.resize) # 테스트 데이터셋의 경우 마스크가 없으므로 0으로 채워진 마스크를 만듭니다.\n","        # 이미지, 마스크, 파일이름, 이미지 원본 사이즈 순으로 반환합니다.\n","        return image, mask, filename, image_size"]},{"cell_type":"markdown","metadata":{"id":"OKN3-Yx1B8ff"},"source":["###  모델 학습과 검증을 수행합니다."]},{"cell_type":"code","source":["import shutil\n","\n","# 학습된 모델을 저장할 경로를 설정합니다.\n","save_directory = f'/content/drive/MyDrive/Crack_Segmentation/results/{TRAIN_NAME}'\n","\n","\n","# 경로를 생성합니다.\n","os.makedirs(save_directory, exist_ok=True)\n","os.makedirs(save_directory+'/model', exist_ok=True)\n","os.makedirs(save_directory+'/pred', exist_ok=True)"],"metadata":{"id":"QXPRdXp6Frez","executionInfo":{"status":"ok","timestamp":1700467747453,"user_tz":-540,"elapsed":2038,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYixTJ0oB8fg","executionInfo":{"status":"ok","timestamp":1700467568407,"user_tz":-540,"elapsed":914,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"b700ba5f-93f2-43c9-e000-ef423938d087"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["from sklearn.model_selection import KFold\n","import torch.optim as optim\n","\n","split = 2\n","kfold = KFold(n_splits=split, shuffle=False)\n","\n","# 어떤 장치에서 학습할지 결정합니다. cuda는 gpu를 사용한다는 것을 의미합니다.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","source":["def calc_iou(mask, pred_mask): # assumes mask, pred_mask are in batch(that is, shape = [B, H, W])\n","    # calculates iou only for value 1(crack)\n","    inter_mask = mask * pred_mask\n","    inter_mask_sum = inter_mask.sum(axis = 0)\n","    mask_sum = mask.sum(axis = 0)\n","    pred_mask_sum = pred_mask.sum(axis = 0)\n","    iou_list = inter_mask_sum / (mask_sum + pred_mask_sum - inter_mask_sum) # n(A \\Union B) = n(A) + n(B) - n(A \\Intersection B)\n","    print(iou_list)\n","    return iou_list.mean()\n","\n"],"metadata":{"id":"GiB_zIJvISqd","executionInfo":{"status":"ok","timestamp":1700475101733,"user_tz":-540,"elapsed":4,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["A = torch.Tensor([\n","    [\n","        [1, 1],\n","        [1, 0]\n","    ],\n","    [\n","        [1, 0],\n","        [0, 0]\n","    ]\n","])\n","B = torch.Tensor([\n","    [\n","        [1, 1],\n","        [1, 0]\n","    ],\n","    [\n","        [1, 1],\n","        [1, 0]\n","    ]\n","])\n","calc_iou(A, B)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9XpPD20MeDZ","executionInfo":{"status":"ok","timestamp":1700475103606,"user_tz":-540,"elapsed":4,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"bc0e4a22-89ab-47dd-f267-60f9c025cd08"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.5000],\n","        [0.5000,    nan]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(nan)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":825,"referenced_widgets":["5f0839d5a3b64a7e86a25756e4c289e2","062d2793c82b43ee8934d8892d554856","ecf1fb97c94740d4b3ca541e27b923ab","de20dbb891ed486ea9f1e65cb9fd9e0f","8c2c779384cc4ca58976767f368d19ad","8953f02964da46fd8a373bcd28e1e5aa","371f80932a464316be957b3e96e73833","0b2bb2bda5a8487eb3e7791709364e1c","4e5959bd397f4dda833c3c604554c6a7","6c9a4fb10b4c4c76bcb4b94e8e8fe9e3","6800b3941fd5413890e6c9607d08b898","6bf5de93d27f4abf988320fd7c703f03","d1378300b19341bba95f704a49f303c1","d1d06a194cd54f86bca7b79a68e63641","89c3426dcc5f4f25a727f7e61732f7dd","6ab66843ac2d4acf8d5977c8fb1232c1"]},"id":"Iw-jRaZgB8fg","executionInfo":{"status":"ok","timestamp":1700467781671,"user_tz":-540,"elapsed":30249,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"739c911a-9e7d-4962-b538-9b171740fce1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:v1th6sjd) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.145 MB of 0.145 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f0839d5a3b64a7e86a25756e4c289e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B sync reduced upload amount by 4.5%             "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁</td></tr><tr><td>val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.49447</td></tr><tr><td>val_acc</td><td>0.87029</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">testfold_0</strong> at: <a href='https://wandb.ai/rokafai/crack_segmentation/runs/v1th6sjd' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation/runs/v1th6sjd</a><br/> View job at <a href='https://wandb.ai/rokafai/crack_segmentation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE1ODIwNA==/version_details/v6' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE1ODIwNA==/version_details/v6</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231120_080619-v1th6sjd/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:v1th6sjd). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231120_080922-tfoedozc</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/rokafai/crack_segmentation/runs/tfoedozc' target=\"_blank\">testfold_0</a></strong> to <a href='https://wandb.ai/rokafai/crack_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/rokafai/crack_segmentation' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/rokafai/crack_segmentation/runs/tfoedozc' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation/runs/tfoedozc</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Fold : 0 epoch: 0 | train: 100%|██████████| 7/7 [00:02<00:00,  3.44it/s]\n","Fold : 0 epoch: 0 | val: 100%|██████████| 7/7 [00:01<00:00,  5.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold : 0, 에폭 : 0, 학습 정확도 : 0.8133746555873326, 검증 정확도 : 0.9237478801182338\n","Fold : 0, Best performance at epoch 0 : 0.9237478801182338\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:tfoedozc) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5959bd397f4dda833c3c604554c6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁</td></tr><tr><td>val_acc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.81337</td></tr><tr><td>val_acc</td><td>0.92375</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">testfold_0</strong> at: <a href='https://wandb.ai/rokafai/crack_segmentation/runs/tfoedozc' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation/runs/tfoedozc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231120_080922-tfoedozc/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:tfoedozc). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231120_080939-yt7jeojo</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/rokafai/crack_segmentation/runs/yt7jeojo' target=\"_blank\">testfold_1</a></strong> to <a href='https://wandb.ai/rokafai/crack_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/rokafai/crack_segmentation' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/rokafai/crack_segmentation/runs/yt7jeojo' target=\"_blank\">https://wandb.ai/rokafai/crack_segmentation/runs/yt7jeojo</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Fold : 1 epoch: 0 | train: 100%|██████████| 7/7 [00:02<00:00,  2.58it/s]\n","Fold : 1 epoch: 0 | val: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold : 1, 에폭 : 0, 학습 정확도 : 0.8012298856462751, 검증 정확도 : 0.9526435307094029\n","Fold : 1, Best performance at epoch 0 : 0.9526435307094029\n"]}],"source":["# 필요한 라이브러리를 임포트합니다.\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","\n","num_epochs = 1  #학습할 에폭의 수를 결정합니다.\n","for i, (train_index, test_index) in enumerate(kfold.split(train_image_paths, train_mask_paths)):\n","    # WandB 사용을 위한 초기 설정을 합니다.\n","    wandb.init(\n","        project=\"crack_segmentation\",\n","        name=f\"{TRAIN_NAME}fold_{i}\",\n","        config={\n","            \"encoder_name\" : 'timm-efficientnet-b0',\n","            'encoder_weights' : 'imagenet',\n","            \"learning_rate\": 0.02,\n","            \"epochs\": 10,\n","            \"batch_size\": 16,\n","            'learning_rate':0.01\n","        })\n","\n","    config = wandb.config\n","    # 데이터 split\n","\n","    fold_train_image_paths = np.array(train_image_paths)[train_index].tolist()[:100]\n","    fold_train_mask_paths = np.array(train_mask_paths)[train_index].tolist()[:100]\n","    fold_valid_image_paths = np.array(train_image_paths)[test_index].tolist()[:100]\n","    fold_valid_mask_paths = np.array(train_mask_paths)[test_index].tolist()[:100]\n","\n","    # 데이터셋 & 데이터 로더 생성\n","    train_dataset = SegDataset(fold_train_image_paths, fold_train_mask_paths, mode='train') # 학습 (train) 데이터 인스턴스 생성\n","    val_dataset = SegDataset(fold_valid_image_paths, fold_valid_mask_paths, mode='val') # 검증 (valid) 데이터 인스턴스 생성\n","    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2) # 학습 데이터 로더\n","    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2) # 검증 데이터 로더\n","\n","    # 모델 & optimizer & loss function 설정\n","    model = smp.Unet(classes=2,\n","                    encoder_name = config['encoder_name'],\n","                    encoder_weights = config['encoder_weights']).to(device)\n","\n","    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])\n","\n","    loss_function = nn.CrossEntropyLoss()\n","\n","    best_iou = 0\n","\n","    for epoch in range(num_epochs):\n","\n","        # 학습을 수행합니다.\n","        model.train() # 모델을 학습 모드로 설정합니다.\n","\n","\n","        # 학습 데이터에 대한 정답율을 기록할 리스트를 선언합니다.\n","        train_losses = []\n","\n","        # 학습 데이터셋으로부터 데이터를 가져옵니다.\n","        for image, mask, filename, image_size in tqdm(train_loader, position=0, leave=True, desc=f'Fold : {i} epoch: {epoch} | train'):\n","\n","            # 가져온 데이터를 장치에 할당합니다.\n","            image = image.to(device) # 이미지의 차원은 [배치사이즈, 채널 수, 이미지 높이, 이미지 넓이]입니다.\n","            mask = mask.to(device) # 마스크의 차원은 [배치사이즈, 이미지 높이, 이미지 넓이]입니다.\n","\n","            # 모델의 출력값을 계산합니다.\n","            # 예측 결과인 pred_mask는 [배치사이즈, 클래스 수, 이미지 높이, 이미지 넓이]차원의 행렬 형태를 가집니다.\n","            pred_mask = model(image)\n","\n","            # loss를 계산합니다.\n","            loss = loss_function(pred_mask, mask)\n","            train_losses.append(loss.mean())\n","\n","            # loss를 통해 모델을 학습합니다.\n","            optimizer.zero_grad() # gradient를 초기화합니다.\n","            loss.backward() # gradient를 계산합니다.\n","            optimizer.step() # gradient를 통해 파라미터를 업데이트합니다.\n","\n","            # argmax 연산을 통해 확률이 가장 높은 클래스를 예측값으로 선택합니다.\n","            # [배치사이즈, 클래스 수, 이미지 높이, 이미지 넓이] -> [배치사이즈, 이미지 높이, 이미지 넓이]로 변환됩니다.\n","            pred_mask = torch.argmax(pred_mask, dim=1)\n","\n","        # 검증을 수행합니다.\n","        model.eval() # 모델을 검증 모드로 설정합니다.\n","\n","        # 검증 데이터에 대한 IOU를 기록할 리스트를 선언합니다.\n","        val_ious = []\n","\n","        # 검증 데이터셋으로부터 데이터를 가져옵니다.\n","        for image, mask, filename, image_size in tqdm(val_loader, position=0, leave=True, desc=f'Fold : {i} epoch: {epoch} | val'):\n","\n","                # 가져온 데이터를 장치에 할당합니다.\n","                image = image.to(device) # 이미지의 차원은 [배치사이즈, 채널 수, 이미지 높이, 이미지 넓이]입니다.\n","                mask = mask.to(device) # 마스크의 차원은 [배치사이즈, 이미지 높이, 이미지 넓이]입니다.\n","\n","                # 모델의 출력값을 계산합니다.\n","                # 예측 결과인 pred_mask는 [배치사이즈, 클래스 수, 이미지 높이, 이미지 넓이]차원의 행렬 형태를 가집니다.\n","                pred_mask = model(image)\n","\n","                # argmax 연산을 통해 확률이 가장 높은 클래스를 예측값으로 선택합니다.\n","                # [배치사이즈, 클래스 수, 이미지 높이, 이미지 넓이] -> [배치사이즈, 이미지 높이, 이미지 넓이]로 변환됩니다.\n","                pred_mask = torch.argmax(pred_mask, dim=1)\n","\n","                # 해당 배치에 대한 성능 평가를 진행합니다.\n","                batch_iou =  #TODO\n","\n","                val_ious.append(batch_iou)\n","\n","        train_loss = sum(train_losses) / len(train_loss)\n","        val_iou = sum(val_ious) / len(val_iou)\n","\n","        wandb.log({'train_loss' : train_loss, 'val_iou': val_iou})\n","\n","        print(f'Fold : {i}, 에폭 : {epoch}, 학습 loss : {train_loss}, 검증 정확도 : {val_iou}')\n","\n","        # validation accuracy가 최고를 갱신하면 모델을 저장합니다.\n","        if val_iou > best_iou: # 이번 에폭의 기존의 최고 accuracy보다 높으면\n","            best_acc = val_iou # 최고 accuracy를 갱신하고\n","            torch.save(model.state_dict(), f'{save_directory}/model/best_model_fold_{i}.pt') # 모델을 저장합니다.\n","            print(f'Fold : {i}, Best performance at epoch {epoch} : {best_iou}')\n"]},{"cell_type":"markdown","metadata":{"id":"UJk56-MXB8fj"},"source":["# 추론을 수행합니다."]},{"cell_type":"code","source":["# 예측 결과를 저장할 경로를 생성합니다.\n","pred_save_directory = save_directory + \"/pred\"\n","\n","# 경로를 생성합니다.\n","os.makedirs(pred_save_directory, exist_ok=True)"],"metadata":{"id":"Q5wY2CSkHLqr","executionInfo":{"status":"ok","timestamp":1700467821435,"user_tz":-540,"elapsed":1740,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cD2qDRS3B8fj","executionInfo":{"status":"ok","timestamp":1700467823076,"user_tz":-540,"elapsed":1644,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"outputs":[],"source":["# 학습이 완료된 모델을 로드합니다.\n","model = smp.Unet(classes=2, # 의류 종류 수에 맞게 클래스를 결정합니다.\n","                 encoder_name = 'timm-efficientnet-b0', # encoder 모델 구조를 결정합니다.\n","                 encoder_weights = 'imagenet') # encoder 모델에 어떤 사전학습 모델을 적용할지 지정합니다.\n","\n","model = model.to(device)"]},{"cell_type":"code","source":["# 데이터셋 & 데이터로더\n","test_dataset = SegDataset(test_image_paths, mode='test') # 평가 (test) 데이터 인스턴스 생성\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2) # 평가 데이터 로더"],"metadata":{"id":"0T56htqGHQR1","executionInfo":{"status":"ok","timestamp":1700467824173,"user_tz":-540,"elapsed":1105,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKqjgmvGB8fj","executionInfo":{"status":"ok","timestamp":1700468247410,"user_tz":-540,"elapsed":423240,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}},"outputId":"c2b73392-b1fa-46e2-8f33-30a951f50669"},"outputs":[{"output_type":"stream","name":"stderr","text":["Prediction: 100%|██████████| 250/250 [07:00<00:00,  1.68s/it]\n"]}],"source":["for image, _, filename, image_size in tqdm(test_loader, position=0, leave=True, desc=f'Prediction'):\n","    # 가져온 데이터를 장치에 할당합니다.\n","    image = image.to(device)\n","\n","    # 모델의 출력값을 계산합니다.\n","    for fold in range(split):\n","        model.load_state_dict(torch.load(f'{save_directory}/model/best_model_fold_{fold}.pt'))\n","        if fold == 0:\n","            pred_mask = model(image)\n","        else:\n","            pred_mask += model(image)\n","\n","    # argmax 연산을 통해 확률이 가장 높은 클래스를 예측값으로 선택합니다.\n","    pred_mask = torch.argmax(pred_mask, dim=1)\n","\n","    for i, a_pred_mask in enumerate(pred_mask):\n","        # pred_mask를 PIL image로 변환합니다.\n","        pred_mask_image = Image.fromarray(np.uint8(a_pred_mask.cpu().numpy()))\n","\n","        # 원본 이미지의 크기로 resize합니다.\n","        pred_mask_image = pred_mask_image.resize(image_size[i], resample=Image.NEAREST)\n","\n","        filename_ = filename[i].replace('.jpg', '.png')\n","\n","\n","        # 이미지를 저장합니다.\n","        pred_mask_image.save(f'{pred_save_directory}/{filename_}')"]},{"cell_type":"markdown","metadata":{"id":"1_knW7eEB8fk"},"source":["### 예측 결과물을 압축해서 제출합니다."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"HdsMx39NB8fk","executionInfo":{"status":"ok","timestamp":1700468259616,"user_tz":-540,"elapsed":12214,"user":{"displayName":"Matia Yi","userId":"13336364829956458208"}}},"outputs":[],"source":["import zipfile\n","\n","pred_files = glob(f'{pred_save_directory}/*.png')\n","\n","# 압축을 수행합니다.\n","with zipfile.ZipFile(save_directory+'/out-of-fold.zip', 'w') as zip:\n","    for pred_file in pred_files:\n","        zip.write(pred_file, os.path.basename(pred_file))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5f0839d5a3b64a7e86a25756e4c289e2":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_062d2793c82b43ee8934d8892d554856","IPY_MODEL_ecf1fb97c94740d4b3ca541e27b923ab"],"layout":"IPY_MODEL_de20dbb891ed486ea9f1e65cb9fd9e0f"}},"062d2793c82b43ee8934d8892d554856":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c2c779384cc4ca58976767f368d19ad","placeholder":"​","style":"IPY_MODEL_8953f02964da46fd8a373bcd28e1e5aa","value":"0.190 MB of 0.190 MB uploaded (0.008 MB deduped)\r"}},"ecf1fb97c94740d4b3ca541e27b923ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_371f80932a464316be957b3e96e73833","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b2bb2bda5a8487eb3e7791709364e1c","value":1}},"de20dbb891ed486ea9f1e65cb9fd9e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c2c779384cc4ca58976767f368d19ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8953f02964da46fd8a373bcd28e1e5aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"371f80932a464316be957b3e96e73833":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b2bb2bda5a8487eb3e7791709364e1c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e5959bd397f4dda833c3c604554c6a7":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6c9a4fb10b4c4c76bcb4b94e8e8fe9e3","IPY_MODEL_6800b3941fd5413890e6c9607d08b898"],"layout":"IPY_MODEL_6bf5de93d27f4abf988320fd7c703f03"}},"6c9a4fb10b4c4c76bcb4b94e8e8fe9e3":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1378300b19341bba95f704a49f303c1","placeholder":"​","style":"IPY_MODEL_d1d06a194cd54f86bca7b79a68e63641","value":"0.011 MB of 0.011 MB uploaded\r"}},"6800b3941fd5413890e6c9607d08b898":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c3426dcc5f4f25a727f7e61732f7dd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ab66843ac2d4acf8d5977c8fb1232c1","value":1}},"6bf5de93d27f4abf988320fd7c703f03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1378300b19341bba95f704a49f303c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d06a194cd54f86bca7b79a68e63641":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89c3426dcc5f4f25a727f7e61732f7dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ab66843ac2d4acf8d5977c8fb1232c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}